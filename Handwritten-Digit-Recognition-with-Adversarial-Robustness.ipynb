{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5b6306-1057-4574-8e33-880e201e6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35794ed7-5431-42e3-ae25-4fa080b859d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65b462a-a365-4bea-933c-6e2a74af3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4934280-586b-4b0c-8180-758ecfd25d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "trainset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the test dataset\n",
    "testset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Combine the trainset and testset\n",
    "mnist_dataset = ConcatDataset([trainset, testset])\n",
    "data_loader = DataLoader(mnist_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b179ec27-32c9-4688-9205-268beb1ed8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform PGD attack on GPU\n",
    "def pgd_attack(model, images, labels, epsilon=0.5, alpha=0.04, iters=10):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    perturbed_images = images.clone().detach().requires_grad_(True).to(device)\n",
    "    \n",
    "    for _ in range(iters):\n",
    "        outputs = model(perturbed_images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perturbation step\n",
    "        with torch.no_grad():\n",
    "            perturbed_images = perturbed_images + alpha * perturbed_images.grad.sign()\n",
    "            eta = torch.clamp(perturbed_images - images, min=-epsilon, max=epsilon)\n",
    "            perturbed_images = torch.clamp(images + eta, min=0, max=1).detach_()\n",
    "        \n",
    "        perturbed_images.requires_grad = True  # Re-enable gradients for next iteration\n",
    "    \n",
    "    return perturbed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96bc84b1-8b08-4181-a7fb-3a5fefbb89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model to compute gradients (dummy model for PGD perturbation)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(10 * 24 * 24, 10)  # Adjusted to match MNIST feature dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, 10 * 24 * 24)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and move to GPU\n",
    "model = SimpleCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b719d0-4c9f-4b1b-8e91-55d7ac178656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturb the images in the dataset\n",
    "perturbed_features = []\n",
    "original_labels = []\n",
    "\n",
    "for images, labels in data_loader:\n",
    "    # Perform PGD attack on the batch of images\n",
    "    perturbed_images = pgd_attack(model, images, labels)\n",
    "    \n",
    "    # Append perturbed features and original labels\n",
    "    perturbed_features.append(perturbed_images.cpu())  # Move back to CPU for storage\n",
    "    original_labels.append(labels.cpu())\n",
    "\n",
    "# Combine all batches into a single dataset\n",
    "perturbed_features = torch.cat(perturbed_features)\n",
    "original_labels = torch.cat(original_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54250da0-cbfb-4c2c-b3a9-ce596018de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset with perturbed images and original labels\n",
    "new_dataset = TensorDataset(perturbed_features, original_labels)\n",
    "\n",
    "# DataLoader for the new perturbed dataset\n",
    "new_data_loader = DataLoader(new_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a0c9f6-e8d0-446b-ab78-f2f74e5e1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels from new_dataset\n",
    "features = perturbed_features  # Input features (images)\n",
    "labels = original_labels  # Target labels\n",
    "\n",
    "# Detach the tensors from the computation graph before converting to NumPy arrays\n",
    "features_np = features.detach().numpy()  # Convert to NumPy arrays after detaching\n",
    "labels_np = labels.detach().numpy()  # Same for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d642926-a9d9-4b36-88bc-d2b2a7bc9901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train-test split (80% train, 20% test, for example)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_np, labels_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert back to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "X_test_tensor = torch.tensor(X_test)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "y_test_tensor = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9b532fb-f326-42bb-8a3e-570055482ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset for train and test sets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader for train and test sets\n",
    "trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d6eeafd-8801-4cfc-b9ea-a90d68617280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output Layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29a51d74-8368-4bf9-a076-4a961fcf84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55497ef4-516e-4c15-a4c3-41f835d130ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1cd08c9-5590-4848-a275-654cc972bba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3153\n",
      "Epoch [2/10], Loss: 0.1797\n",
      "Epoch [3/10], Loss: 0.0134\n",
      "Epoch [4/10], Loss: 0.0009\n",
      "Epoch [5/10], Loss: 0.0000\n",
      "Epoch [6/10], Loss: 0.0000\n",
      "Epoch [7/10], Loss: 0.0000\n",
      "Epoch [8/10], Loss: 0.0000\n",
      "Epoch [9/10], Loss: 0.0000\n",
      "Epoch [10/10], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d525a4d4-5ca1-4fda-94d0-51f5df6ee1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Perturbed Dataset: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on Perturbed Dataset: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "192e8512-c0e7-4e27-a31d-5350fb2c3530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Clean up the dataset directory\n",
    "shutil.rmtree('./data')  # This will delete the 'data' directory and its contents\n",
    "print(\"Dataset deleted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d737d6-bd65-4f02-976b-03d2d96cdf1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
